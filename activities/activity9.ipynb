{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(activity9)=\n",
    "# Activity 9: Positivity in Yeager et al. 2019\n",
    "\n",
    "**2025-10-03**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and examine the data\n",
    "learning_df = pd.read_csv(\"~/COMSC-341CD/data/learning_mindset.csv\")\n",
    "learning_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This selected portion of the National Study of Learning Mindsets dataset is not truly randomized, so we'll need to adjust for confounding.\n",
    "\n",
    "The columns we will look at are:\n",
    "\n",
    "- `intervention`: whether the student received the intervention (1) or not (0)\n",
    "- `success_expect`: student prior mindset about their ability to succeed in school (higher values indicate a stronger belief in their ability to succeed)\n",
    "- `frst_in_family`: whether the student would be the first in their family to attend college (1) or not (0)\n",
    "- `gender`: student's self-reported gender\n",
    "- `school_urbanicity`: categorical variable corresponding to the urbanicity of the school the student attends, e.g. urban, suburban, rural\n",
    "- `achievement_score`: the student's future grade achievement, standardized such that 0 is the mean and it has a standard deviation of 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Imbalance in covariates\n",
    "\n",
    "In this vesrion of the dataset we are anlayzing, there appear to be some differences in the distribution of covariates between the treatment and control groups where participants have different probabilities of receiving the intervention.\n",
    "\n",
    "## 1.1\n",
    "\n",
    "Perform two separate `groupby` operations to compute the mean of `intervention`, grouping by: \n",
    "\n",
    "- `success_expect`\n",
    "- `frst_in_family`\n",
    "\n",
    "What do you observe? Are certain groups of students more or less likely to receive the growth mindset video? Keep in mind that `intervention=1` corresponds to the growth mindset video intervention.\n",
    "\n",
    "**Your response**: TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO perform two separate groupby operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Examining positivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates = ['success_expect', 'frst_in_family']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seeen some potential confounding in `success_expect` and `frst_in_family`, let's try to control for them. If we take the same strategy as we have done before with stratification, we'll need to bin on the confounders and compute treatment effects for each bin.\n",
    "\n",
    "However, we also need to be careful about positivity violations. First, let's compute the total number of bins we need to create if we want to control for these two covariates.\n",
    "\n",
    "We can do this by using [pd.Series.nunique](https://pandas.pydata.org/docs/reference/api/pandas.Series.nunique.html) to get the number of unique values for each covariate and then multiplying them together. This is like counting the number of possible bins over the all possible combinations of each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO calculate the total number of bins \n",
    "total_bins = 0\n",
    "\n",
    "print(f\"Total number of bins: {total_bins}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's check if positivity holds. We can do this by grouping over the covariates plus the intervention, and then counting the number of unique groups are actually present in the data.\n",
    "\n",
    "To generate the per-bin counts, we perform a `groupby(all_cols, as_index=False)` over the intervention and all combinations of the other columns, and the check the `ngroups` attribute of the resulting groupby object. How many groups are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the intervention column and the two covariates\n",
    "all_cols = []\n",
    "#group_count = learning_df.groupby(all_cols, as_index=False).ngroups\n",
    "\n",
    "\n",
    "print(f\"Number actual groups among the bins for {all_cols}: {group_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we need each bin to have both control and treatment units in order to have a valid comparison, the total number of groups should be equal to **2 times the total number of bins possible** for there to be no positivity violations.\n",
    "\n",
    "Does the number of groups you found in 2.1 match this?\n",
    "\n",
    "**Your response**: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally we'd like to control for as many confounders as possible to make conditional exchangeability more plausible. Let's now add `gender` and `school_urbanicity` to our list of covariates, making a total of 4 confounders.\n",
    "\n",
    "Repeat the analysis above with the new set of covariates. Do we see positivity violations with the new set of covariates?\n",
    "\n",
    "**Your response**: TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO your code here \n",
    "\n",
    "\n",
    "print(f\"Number actual groups among the bins for {all_cols}: {group_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Optional extra\n",
    "\n",
    "\n",
    "if we actually want to see the bins that are missing, we can generate a [pivot_table](https://pandas.pydata.org/docs/reference/api/pandas.pivot_table.html) of the counts, and then identify the bins that are missing in either the control or treatment group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df = learning_df.groupby(all_cols, as_index=False).size()\n",
    "\n",
    "# Create a pivot table to show counts by intervention and bins\n",
    "bin_pivot = pd.pivot_table(\n",
    "    count_df, \n",
    "    index=['success_expect', 'gender', 'frst_in_family', 'school_urbanicity'],\n",
    "    columns=['intervention'],\n",
    "    values='intervention',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Display information about the pivot table\n",
    "print(\"Bins with no control units:\")\n",
    "display(bin_pivot[bin_pivot[0] == 0])\n",
    "\n",
    "print(\"Bins with no treatment units:\")\n",
    "display(bin_pivot[bin_pivot[1] == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# References\n",
    "\n",
    "- Yeager, D. S. et al. (2019). A national experiment reveals where a growth mindset improves achievement. Nature.\n",
    "- Athey, S., & Wager, S. (2019). Estimating treatment effects with causal forests: An application. Observational studies.\n",
    "- Facure, M. (2023). Causal Inference for the Brave and the True."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
