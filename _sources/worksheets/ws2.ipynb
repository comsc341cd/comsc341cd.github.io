{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ws2)=\n",
    "# Worksheet 2 🐼\n",
    "\n",
    ":::{epigraph}\n",
    "Tabular Data and Potential Outcomes\n",
    "\n",
    "-- TODO your name here\n",
    ":::\n",
    "\n",
    ":::{admonition} Collaboration Statement\n",
    "- TODO brief statement on the nature of your collaboration.\n",
    "- TODO your collaborator's names here.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Objectives\n",
    "\n",
    "- Practice tabular data manipulation through pandas\n",
    "- Learn more about the potential outcomes framework for causal inference\n",
    "- Familiarization with mathematical operations on potential outcomes\n",
    "- Familiarization with Gradescope's autograder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note} Before you begin\n",
    "Please go to `Edit -> Clear Outputs of All Cells` so you can run the code cells yourself!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. `pandas` and tabular data [2 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To familiarize ourselves with working with tabular data, we will use the [pandas](https://pandas.pydata.org/) library and work with the [National Health and Nutrition Examination Survey (NHANES) dataset](https://www.cdc.gov/nchs/nhanes/about/?CDC_AAref_Val=https://www.cdc.gov/nchs/nhanes/about_nhanes.htm). NHANES is a large, national survey that measures the health and nutrition of adults and children in the United States.\n",
    "\n",
    "The NHANES dataset is frequently used in **observational studies**, where there is no intervention or manipulation of the data like a randomized experiment. We'll cover these type of studies in a few weeks. For now, we'll use pandas to answer some **descriptive** questions about the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pandas` basics\n",
    "\n",
    "Pandas is the defacto Python framework for working with tabular data, and it is supported by a large ecosystem of libraries, including seamless integration with numpy. Pandas provides two main data structures:\n",
    "\n",
    "- `DataFrame`: a 2-dimensional data structure often used to represent a table with rows and named columns.\n",
    "- `Series`: a 1-dimensional, **labelled** array, often used to represent a single column or row in a `DataFrame`\n",
    "\n",
    "Pandas dataframes allow for the datatypes of the columns to be mixed. Let's create a couple of examples by running the code cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# the standard import idiom for pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a series from a list, note how there can 'nan' (missing) values\n",
    "review_scores = pd.Series([4, np.nan, 2, 3])\n",
    "\n",
    "# Note that the series can be of different data types\n",
    "review_text = pd.Series(['I liked it', 'It was awful', 'Bland', 'Pretty good'])\n",
    "\n",
    "# pandas will automatically assign a data type (dtype) to the series\n",
    "review_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "`np.nan` is a special value in pandas that represents missing data. A critical part of the data science workflow in practice is handling missing data, as results can change dramatically when missing data is not handled properly.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then create a dataframe by passing in a dictionary of series, where the keys are the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'review_scores': review_scores, 'review_text': review_text})\n",
    "\n",
    "# Print the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can create a dataframe from a dictionary of lists, which can sometimes be convenient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code produces the same dataframe as before\n",
    "df = pd.DataFrame({'review_scores': [4, np.nan, 2, 3], \n",
    "                   'review_text': ['I liked it', 'It was awful', 'Bland', 'Pretty good']})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For high-level inspection of the dataframe, we can use the following functions and attributes:\n",
    "\n",
    "- `df.head()`: returns the first 5 rows of the dataframe\n",
    "- `df.tail()`: returns the last 5 rows of the dataframe\n",
    "- `df.info()`: returns a summary of the dataframe, including the number of rows, columns, and the data types of each column\n",
    "- `df.columns`: returns the column names of the dataframe\n",
    "- `df.shape`: returns the number of rows and columns in the dataframe\n",
    "- `df.dtypes`: returns the data types of each column\n",
    "\n",
    "You can play around with the dataframe in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the shape is a tuple of (num rows, num columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Often times we'll be working with pre-existing datasets. Here, we'll use a pre-loaded version of the NHANES dataset, which is available in the `nhanes` package. \n",
    "\n",
    ":::{note}\n",
    "To load data from files manually, pandas provides various `pd.read_*` functions. For example, [pd.read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) loads (comma-separated values) CSV files. See pandas' [I/O documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html) for more options. However, for the purposes of this worksheet, we'll use a 3rd party package to load in NHANES because it is a complex dataset.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the NHANES data and metadata using pre-built functions\n",
    "from nhanes.load import load_NHANES_data, load_NHANES_metadata\n",
    "\n",
    "nhanes_df = load_NHANES_data()\n",
    "\n",
    "nhanes_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, we see that the dataframe has 8,366 rows and 197 columns. Here, each row represents a survey respondent, and each column represents a survey question that was asked.\n",
    "\n",
    "Taking a look at the columns, we see that there are a wide range of health-related questions that were asked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .values returns the column names as a numpy array\n",
    "# nhanes_df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select a single column, we can square bracket indexing, as if we were acessing a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects the 'GeneralHealthCondition' column and prints the first 5 rows\n",
    "nhanes_df['GeneralHealthCondition'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When initially exploring a dataset, it is often useful to see the unique values in a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique values in the 'GeneralHealthCondition'\n",
    "nhanes_df['GeneralHealthCondition'].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice two things about the `GeneralHealthCondition` column:\n",
    "\n",
    "- There are some missing values in the column, represented as `nan` \n",
    "- There are two values that seem to be typos: `Poor?` and `Fair or`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify our analysis, we can remove the rows with missing values via [dropna()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with missing values in the 'GeneralHealthCondition' column, resulting in a new dataframe\n",
    "nhanes_df = nhanes_df.dropna(subset=['GeneralHealthCondition'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also correct the typos using [replace()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the typos with the correct values\n",
    "nhanes_df['GeneralHealthCondition'] = nhanes_df['GeneralHealthCondition'].replace({'Poor?': 'Poor', 'Fair or': 'Fair'})\n",
    "\n",
    "# print the unique values again to see the changes\n",
    "nhanes_df['GeneralHealthCondition'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{tip}\n",
    "Notice how in both operations, we reassign the result back to `nhanes_df` or a column in `nhanes_df`, which is a common pattern in pandas -- operations often return a new dataframe or column, and we need to reassign the result back to the original variable if we want to keep the changes.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing and assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The square bracket indexing can be generalized to selecting multiple columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects multiple columns and prints the first 10 rows\n",
    "cols = ['GeneralHealthCondition', 'AgeInYearsAtScreening', 'SmokedAtLeast100CigarettesInLife', 'WeightKg', 'VigorousRecreationalActivities']\n",
    "nhanes_df[cols].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like numpy, we can also use boolean indexing to select portions of the dataframe based on a condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects respondents who have smoked at least 100 cigarettes in their life\n",
    "sel_df = nhanes_df[nhanes_df['SmokedAtLeast100CigarettesInLife'] == 1]\n",
    "\n",
    "sel_df[cols].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the `value_counts` function to get the frequency of each category in a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sel_df['GeneralHealthCondition'].value_counts())\n",
    "\n",
    "# normalize=True to get the proportion of each category\n",
    "# dropna=False to include missing values as part of counts\n",
    "print(sel_df['GeneralHealthCondition'].value_counts(dropna=False, normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{tip}\n",
    "Many pandas operations by default drop na values, so be careful to double check what the default behavior is!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1**. How would we get the general health condition proportions of respondents who have engaged in vigorous recreational activities (1 = yes, 0 = no)? Write code to do this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What proportion of respondents who have engaged in vigorous recreational activities report being in excellent health?\n",
    "\n",
    "**Your response**: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These boolean conditions can be combined using the `&` (AND), `|` (OR), and `~` (NOT) operators. Additionally, there are some special functions that can be used to select data based on a condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining conditions: respondents who are 18 or older and have engaged in vigorous recreational activities\n",
    "nhanes_df[(nhanes_df['AgeInYearsAtScreening'] >= 18) & (nhanes_df['VigorousRecreationalActivities'] == 1)] \n",
    "\n",
    "# isin(): select rows where a column is in a list of values\n",
    "nhanes_df[nhanes_df['GeneralHealthCondition'].isin(['Poor?', 'Fair or'])]\n",
    "\n",
    "# isna(): select rows where a column is missing\n",
    "nhanes_df[nhanes_df['SmokedAtLeast100CigarettesInLife'].isna()];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{tip}\n",
    "To avoid errors, always use parentheses when combining conditions:\n",
    "\n",
    "- Incorrect: `df[column1 == value1 & column2 == value2]`\n",
    "- Correct: `df[(column1 == value1) & (column2 == value2)]`\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create new columns or modify existing columns via `=` assignment (like we did with `replace()`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a constant value to a new column where every row has the same value\n",
    "nhanes_df['country'] = 'United States'\n",
    "\n",
    "# Create a new column based on a condition of other columns\n",
    "nhanes_df['is_adult'] = nhanes_df['AgeInYearsAtScreening'] >= 18\n",
    "\n",
    "# Create a new column based on a function of other columns: calculate BMI\n",
    "nhanes_df['bmi'] = nhanes_df['WeightKg'] / (nhanes_df['StandingHeightCm'] / 100) ** 2\n",
    "\n",
    "# Print the updated dataframe\n",
    "nhanes_df[['country', 'is_adult', 'AgeInYearsAtScreening', 'bmi']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2**. Let's practice combining these operations. Select respondents who:\n",
    "\n",
    "- are adults AND\n",
    "- report being in `Excellent` or `Very good` health AND\n",
    "- are NOT missing any values in the `bmi` column \n",
    "\n",
    "Write code to do this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many respondents are in the selected group? How many of them have smoked at least 100 cigarettes in their life?\n",
    "\n",
    "**Your response**: TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the `.loc` operator\n",
    "\n",
    "In addition to square bracket indexing, pandas provides a powerful indexing method via the [.loc](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html) operator, which allows us to select rows and columns by their labels or even boolean conditions.\n",
    "\n",
    "Generally, the `.loc` operator is used to select rows and columns by their labels:\n",
    "\n",
    "```python\n",
    "df.loc[row_label, column_label]\n",
    "```\n",
    "\n",
    "The loc operator borrows from the slice notation of python lists, where we can use `:` to select all rows or column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selects all rows and the 'bmi' column, returned as a series\n",
    "nhanes_df.loc[:, 'bmi'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selects all rows and the ['country', 'is_adult', 'bmi'] columns\n",
    "# Note that the columns are wrapped in a list!\n",
    "nhanes_df.loc[:, ['country', 'is_adult', 'bmi']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where the .loc operator becomes useful is when we want to compactly select rows and columns based on a condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selects rows where the 'bmi' column is greater than 25, and returns the 'bmi' and 'is_adult' columns\n",
    "nhanes_df.loc[nhanes_df['bmi'] > 25, ['bmi', 'is_adult']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This selection can be used to modify columns based on a condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the 'bmi' column to np.nan for all rows where the 'is_adult' column is False\n",
    "nhanes_df.loc[nhanes_df['is_adult'] == False, 'bmi'] = np.nan\n",
    "\n",
    "# print the updated dataframe\n",
    "nhanes_df[['bmi', 'is_adult']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3.** Common age cutoffs that are used in health policy studies are 18 and 65 -- 18 is when people are legally considered adults in the United States, and 65 is when they are eligible for Medicare health insurance. Create a new column `age_category` using the `AgeInYearsAtScreening` column and the `.loc` operator that makes the following assignments:\n",
    "\n",
    "- `'child'`: age < 18\n",
    "- `'adult'`: 18 <= age < 65\n",
    "- `'senior'`: age >= 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many respondents are in the 'senior' age category?\n",
    "\n",
    "**Your response**: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping and transformation\n",
    " \n",
    "The [groupby()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html) function is a powerful tool for performing aggregations on subsets of the dataframe. We pass in one or more columns as the `by` argument, which then divides the original Dataframe based on the unique values of the column(s). We then often apply an **aggregation** function to each group, resulting in a new dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can replicate `value_counts()` with `groupby()` and the `size()` aggregation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of people in each health condition category\n",
    "print(nhanes_df.groupby(by='GeneralHealthCondition').size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute summary statistics like `mean()`, `std()`, `median()`, `min()`, `max()` on each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean BMI by general health condition\n",
    "nhanes_df.groupby(by='GeneralHealthCondition')['bmi'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also group by multiple columns, which then groups the dataframe by the unique cross-product of the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean BMI by general health condition and vigorous recreational activities\n",
    "# as_index=False keeps the groupby columns as columns in the resulting dataframe\n",
    "nhanes_df.groupby(by=['GeneralHealthCondition', 'VigorousRecreationalActivities'], as_index=False)['bmi'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply multiple aggregation functions, we can pass in a dictionary of columns as keys and functions as values to [agg()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.agg.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean, min, max for BMI and median household income by general health condition\n",
    "nhanes_df.groupby(by='GeneralHealthCondition').agg({'bmi': ['mean', 'min', 'max'], 'AnnualHouseholdIncome': 'median'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4**. Generate a table that groups by:\n",
    "\n",
    "- `GeneralHealthCondition`\n",
    "- `SmokedAtLeast100CigarettesInLife`\n",
    "\n",
    "and calculates the mean of the following columns:\n",
    "\n",
    "- `EverToldYouHavePrediabetes`: 0 = no, 1 = yes\n",
    "- `EverToldYouHadHeartAttack`: 0 = no, 1 = yes\n",
    "\n",
    ":::{note}\n",
    "The mean of a binary column is equivalent to the proportion of 1s in the column.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which subgroup of health conditions and smoking status has the highest proportion of people who have been told they have had a heart attack?\n",
    "\n",
    "**Your response**: TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "For more on pandas operations, see [this quickstart guide](https://pandas.pydata.org/docs/user_guide/10min.html) and the associated links within it.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Reading: Neal 2020 [1 pt]\n",
    "\n",
    "Read the following sections of [Neal 2020: An Introduction to Causal Inference](https://www.bradyneal.com/Introduction_to_Causal_Inference-Dec17_2020-Neal.pdf) and answer the questions below:\n",
    "\n",
    "- Sections 2.1 - 2.3.2\n",
    "- Section 2.3.5\n",
    "\n",
    ":::{admonition} Reading Notes\n",
    "\n",
    "- Don't worry too much about the causal graphs shown in the margin, as we'll cover causal graph formalisms in upcoming classes.\n",
    "\n",
    "- The notation $\\triangleq$ is used to denote \"is defined as\". For example, Equation 2.1 $\\tau_i \\triangleq Y_i(1) - Y_i(0)$ means that the individual treatment effect for unit $i$ is defined as $Y_i(1) - Y_i(0)$.\n",
    "\n",
    "- A mathematical identity we'll need for the derivations is **linearity of expectation**. This states that for *any* random variables $X$ and $Y$ and constant $a$: \n",
    "\n",
    "$$\n",
    "E[X + Y] = E[X] + E[Y]\n",
    "$$\n",
    "\n",
    "$$\n",
    "E[aX] = aE[X]\n",
    "$$\n",
    "\n",
    "- Implications of two variables $X$ and $Y$ being independent (written $X \\perp Y$) are that:\n",
    "\n",
    "$$\n",
    "E[X | Y=y] = E[X]\n",
    "$$\n",
    "\n",
    "$$\n",
    "E[XY] = E[X]E[Y]\n",
    "$$\n",
    "\n",
    "- The consistency assumption (Assumption 2.5) allows us to define the observed outcome $Y$ with binary treatment $T$ as follows:\n",
    "\n",
    "$$\n",
    "Y = T \\cdot Y(1) + (1-T) \\cdot Y(0)\n",
    "$$\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**2.1.** State the fundamental problem of causal inference in your own words.\n",
    "\n",
    "**Your Response**: TODO\n",
    "\n",
    "**2.2.** These sections of Neal 2020 cover three key assumptions that allow us to **identify** (definition 2.1, p. 10) the average treatment effect (ATE) of a binary treatment:\n",
    "\n",
    "- ignorability / exchangeability (Assumption 2.1, p. 9)\n",
    "- no interference (Assumption 2.4, p. 13)\n",
    "- consistency (Assumption 2.5, p. 14)\n",
    "\n",
    "Below is a restatement of the ATE derivation. State which mathematical identities and assumptions are used on each line:\n",
    "\n",
    "$$\n",
    "ATE &= E[Y(1) - Y(0)]\\\\\n",
    "    &= E[Y(1)] − E[Y(0)], \\text{ from TODO} \\\\\n",
    "    &= E[Y(1) | T = 1] − E[Y(0) | T = 0], \\text{ from TODO} \\\\\n",
    "    &= E[Y | T = 1] − E[Y | T = 0] \\text{ from TODO}\n",
    "$$\n",
    "\n",
    "**2.3.** The no interference assumption states that the potential outcomes for one unit are unaffected by the treatment assignment of other units. Breaking down Neal's example on p. 13, suppose that you are unit 1 and your friend is unit 2. If your friend gets a dog ($t_2 = 1$), we can then examine your potential outcomes:\n",
    "\n",
    "$$\n",
    "&Y_1(t_1, t_2), \\text{ potential outcomes for unit 1} \\\\ \n",
    "&Y_1(0, 1), \\text{ you don't get a dog, but your friend does} \\\\\n",
    "&Y_1(1, 1), \\text{ you and your friend get a dog}\n",
    "$$\n",
    "\n",
    "The argument is that it is likely that the potential outcomes for your happiness are affected by your friend's choice to get a dog: perhaps you'll already be happier if your friend gets a dog because you get to play with them, or perhaps you're allergic to dogs so you have to see your friend less often, etc. This leads to a violation of the no interference assumption where your potential outcomes are no longer just a function of your own treatment assignment $t_1$:\n",
    "\n",
    "$$\n",
    "Y_1(0, 1) \\neq Y_1(0) \\\\\n",
    "Y_1(1, 1) \\neq Y_1(1)\n",
    "$$\n",
    "\n",
    "Neal then states that \"violations of the no interference assumption are rampant in network data,\" e.g. friend groups like the dog example above, social networks, or transportation networks (think roads, subway lines, airports). Give an example of a setting along with a treatment $T$ and outcome $Y$ where you think the no interference assumption is likely to be violated.\n",
    "\n",
    "**Your Response**: \n",
    "\n",
    "- Treatment: TODO\n",
    "- Outcome: TODO\n",
    "- Reasoning: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Causal inference as a missing data problem [1 pt]\n",
    "\n",
    "Let's now put the conceptual ideas around potential outcomes together with our table manipulation practice in pandas.\n",
    "\n",
    "Our framework for causal inference is that there is there is a causal \"ground truth\" table of potential outcomes for each unit. With our example from class, we might be interested in studying the effect of taking aspirin $T$ ($T=1$ means taking aspirin, $T=0$ means not taking aspirin) on headaches $Y$ ($Y=1$ means having a headache, $Y=0$ means not having a headache). The potential outcomes then correspond to:\n",
    "\n",
    "$$\n",
    "Y(0) = \\text{what the outcome } \\textit{would have been} \\text{ if } T = 0 \\\\\n",
    "Y(1) = \\text{what the outcome } \\textit{would have been} \\text{ if } T = 1\n",
    "$$\n",
    "\n",
    "We then have a table of potential outcomes for each unit, where each row is a unit and each column is a potential outcome:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|c|c|c}\n",
    "\\text{Unit } i & T & Y(0) & Y(1) \\\\\n",
    "\\hline\n",
    "1 & 0 & 1 & 1 \\\\\n",
    "2 & 1 & 1 & 0 \\\\\n",
    "3 & 0 & 1 & 0 \\\\\n",
    "4 & 1 & 0 & 0 \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The problem is that we are not omniscient, nor do we have a time machine, so we only observe one realized outcome $Y$ for each unit. There are thus missing values in the table of the data we actually observe:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|c|c|c|c}\n",
    "\\text{Unit } i & T & Y(0) & Y(1) & Y \\\\\n",
    "\\hline\n",
    "1 & 0 & 1 & \\text{nan} & 1 \\\\\n",
    "2 & 1 & \\text{nan} & 0 & 0 \\\\\n",
    "3 & 0 & 1 & \\text{nan} & 1 \\\\\n",
    "4 & 1 & \\text{nan} & 0 & 0 \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The challenge in causal inference is to try to construct causal estimates when we only have the observed data. Throughout this course we'll use our ability to simulate different data generating processes in code to understand the properties of causal inference methods, and when they can or cannot identify the causal effect.\n",
    "\n",
    "We'll start by writing a utility function that takes treatment assignments and a ground truth table of potential outcomes, and generates the realized outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Hint\n",
    ":class: tip\n",
    "\n",
    "To construct the observed outcome $Y$, look to how we defined $Y$ under the consistency assumption.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_observed_df(po_df, treatments):\n",
    "    \"\"\"\n",
    "    Generates a dataframe of realized outcomes from a ground truth table of potential outcomes and treatment assignments.\n",
    "    The unobserved potential outcomes are set to np.nan.\n",
    "    \n",
    "    Args:\n",
    "        po_df (pd.DataFrame): a dataframe of potential outcomes with columns 'Y0' and 'Y1'\n",
    "        treatments (pd.Series): a series of treatment assignments with the same length as po_df\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: a dataframe of realized outcomes with columns 'T', 'Y0', 'Y1', and 'Y'\n",
    "    \"\"\"\n",
    "    assert po_df.shape[0] == treatments.shape[0], \"The number of rows in po_df and treatments must match\"\n",
    "    \n",
    "    # TODO your code here\n",
    "    observed_df = po_df.copy()\n",
    "\n",
    "    return observed_df\n",
    "\n",
    "# Generate some simple test data, matching the example above\n",
    "treatments = pd.Series([0, 1, 0, 1])\n",
    "po_df = pd.DataFrame({'Y0': [1, 1, 1, 0], 'Y1': [1, 0, 0, 0]})\n",
    "observed_df = generate_observed_df(po_df, treatments)\n",
    "assert 'Y' in observed_df.columns, \"The observed dataframe does not have a 'Y' column\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{tip}\n",
    "This is the first auto-graded function in the course -- there will be no submission limit so feel free to try multiple times. We will frequently provide some simple test cases via `assert` statements to check your function. The syntax for assert statements is as follows:\n",
    "\n",
    "```python\n",
    "assert bool_expression, description\n",
    "```\n",
    "where the `bool_expression` is a boolean expression that tests some condition, and the `description` is a string that will be displayed if the assertion fails (evaluates to `False`).\n",
    "\n",
    "You are encouraged to write your own test cases as well for your functions!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Takeaway\n",
    "\n",
    "This transformation from the ground truth table of potential outcomes to the observed data is a key step in thinking about causal quantities. **If only** we had the ground truth table, we could directly estimate quantities like $E[Y(1)]$ and $E[Y(0)]$. However, since we only have the observed data, we need to make assumptions, like the ones covered in the Neal reading, in order to \"reconstruct\" those quantities. This is the process of **identifying** the causal effect. See the figure below:\n",
    "\n",
    "```{figure} ../images/ws2_hernan_fig_1_1.png\n",
    "---\n",
    "width: 500px\n",
    "name: potential-outcomes-to-observed\n",
    "---\n",
    "Adapted from Hernán and Robins 2020 1.1\n",
    "```\n",
    "\n",
    "We saw from the Neal reading that the assumptions made in a randomized experiment allow for us to identify the causal effect, i.e. reconstruct $E[Y(1)]$ from $E[Y|T=1]$, and $E[Y(0)]$ from $E[Y|T=0]$. \n",
    "You may sometimes hear that \"association is causation, in a randomized experiment.\" \n",
    "\n",
    "The crux of the \"design\" step in the causal roadmap generally involves evaluating what assumptions we need in order to perform identification, and whether those assumptions are reasonable to make.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Reflection [1 pt]\n",
    "\n",
    "**4.1** How much time did it take you to complete this worksheet? How long did it take you to read the selected sections of Neal 2020?\n",
    "\n",
    "**Your Response**: TODO\n",
    "\n",
    "**4.2** What is one thing you have a better understanding of after completing this worksheet and going though the class content this week? This could be about the concepts, the reading, or the code.\n",
    "\n",
    "**Your Response**: TODO\n",
    "\n",
    "**4.3** What questions or points of confusion do you have about the material covered in the past week of class?\n",
    "\n",
    "**Your Response**: TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{tip}\n",
    "Don't forget to check that all of the TODOs are filled in before you submit!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowledgements\n",
    "\n",
    "Portions of this worksheet are adapted from Bhargavi's study notes on pandas. This worksheet also uses Russ Poldrack's [nhanes package](https://github.com/poldrack/nhanes), which processes the NHANES dataset into a convenient pandas dataframe. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
