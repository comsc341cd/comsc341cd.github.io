{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(activity8)=\n",
    "# Activity 8: Propensity Scores for Overlap and Balancing\n",
    "\n",
    "**2025-03-06**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from ipywidgets import interact_manual\n",
    "from ipywidgets import widgets\n",
    "\n",
    "\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Distribution of covariates under true randomization\n",
    "\n",
    "On Tuesday we saw that the `success_expect` covariate was associated with the intervention. What should the distribution of `success_expect` look like under true randomization (coin flips)? And does it matter if the coin flips are \"biased\" (not 50% heads, 50% tails)?\n",
    "\n",
    "First, let's simulate a distribution of `success_expect` under true randomization. Begin by generating some data that represents `success_expect`, which takes on integer values between 1 and 7. Use the [rng.choice](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html) function to generate 10,000 samples from a uniform distribution over the integers 1 through 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "# sim_success_expect = TODO call rng.choice with the a and size parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, generate random treatment assignments for each sample using the `rng.choice` function. For now, assume the probability of treatment is 0.5, that is:\n",
    "\n",
    "$$ \n",
    "P(T=1) = 0.5 \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_treat = 0.5\n",
    "# p needs to be a list of probabilities for each outcome: [0, 1]\n",
    "p = [1-p_treat, p_treat]\n",
    "# sim_coin_flips = TODO call rng.choice with the a, size, and p parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot the distribution of treatment for each `sim_success_expect` level. We'll do this by creating a dataframe with the two columns `sim_success_expect` and `sim_treat`, and then using sns.barplot to plot the distribution of `sim_treat` for each `sim_success_expect` level.\n",
    "\n",
    "Call [sns.barplot](https://seaborn.pydata.org/generated/seaborn.barplot.html) with the x and y parameters set to `sim_success_expect` and `sim_treat`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df = pd.DataFrame({\n",
    "    'sim_success_expect': sim_success_expect, \n",
    "    'sim_treat': sim_treat\n",
    "})\n",
    "\n",
    "# TODO sns.barplot\n",
    "# TODO horizontal line corresponding to p_treat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The black vertical lines on each bar represnt a bootstrapped confidence interval for the proportion of treatment in each `sim_success_expect` level. On your plot above, add a horizontal line using `plt.axhline` at `y=p_treat` to represent the true probability of treatment. The observed probability of treatment shouldn't deviate significantly from the true probability of treatment for any of the `sim_success_expect` values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's explore different different probabilities of treatment. Copy your code from the above three cells that created your barplot into the `plot_treatment_distribution` function below, and add an `interact_manual` decorator that lets you change `p_treat`. \n",
    "\n",
    "Does the probability of treatment change the distribution of treated units among the `sim_success_expect` values? Briefly comment on why/why not.\n",
    "\n",
    "**Your response:** [pollev.comâ€‹/tliu](https://pollev.com/tliu)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO an interact_manual\n",
    "def plot_treatment_distribution(p_treat):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Fitting a Propensity Score\n",
    "\n",
    "Let's now estimate a propensity score for the learning mindsets data. As a reminder from Tuesday's lecture, the columns we'll look at are:\n",
    "\n",
    "- `intervention`: whether the student received the intervention (1) or not (0)\n",
    "- `success_expect`: student prior mindset about their ability to succeed in school (higher values indicate a stronger belief in their ability to succeed)\n",
    "- `frst_in_family`: whether the student would be the first in their family to attend college (1) or not (0)\n",
    "- `gender`: student's self-reported gender\n",
    "- `school_urbanicity`: categorical variable corresponding to the urbanicity of the school the student attends, e.g. urban, suburban, rural\n",
    "- `achievement_score`: the student's future grade achievement, standardized such that 0 is the mean and it has a standard deviation of 1\n",
    "\n",
    "\n",
    "The way we'll do this is by fitting a logistic regression model, which allows us estimate the following:\n",
    "\n",
    "$$\n",
    "P(T=1 \\mid X) = E[T \\mid X] = e(X)\n",
    "$$\n",
    "\n",
    "Like with linear regression, this can extend to multiple covariates, such as:\n",
    "\n",
    "$$\n",
    "P(T=1 \\mid X_1, X_2, X_3, X_4) = E[T \\mid X_1, X_2, X_3, X_4]\n",
    "$$\n",
    "\n",
    "\n",
    "In statmodels, we just need to specify the formula string `'T ~1 + X1 + X2 + X3 + X4'` and pass the dataframe to `smf.logit` instead of `smf.ols` to fit a logistic regression model:\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "model = smf.logit('T ~ 1 + X1 + X2 + X3 + X4', data=data).fit()\n",
    "```\n",
    "\n",
    "Fit a propensity score model predicting `intervention` and our four covariates from Tuesday's lecture (`success_expect`, `gender`, `frst_in_family`, `school_urbanicity`).\n",
    "\n",
    "One additional consideration is that `gender` and `school_urbanicity` are categorical variables. We can indicate to `smf.logit` that these variables are categorical by wrapping the variables in `C()` in the formula string. For example, if we wanted to fit a model with `X1` and `X2` as categorical variables, we would write:\n",
    "\n",
    "```python\n",
    "model = smf.logit('T ~ 1 + C(X1) + C(X2) + X3 + X4', data=data).fit()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the learning_mindset data again\n",
    "learning_df = pd.read_csv(\"~/COMSC-341CD/data/learning_mindset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO fit a propensity score model\n",
    "formula = ''\n",
    "#propensity_model = smf.logit(formula=formula, data=learning_df).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then add the predicted propensity scores as a column to analyze along with the rest of the data. Run the cell below to generate predicted propensity scores and add them to the learning_df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_propensity_scores = propensity_model.predict(learning_df)\n",
    "learning_df['propensity_score'] = pred_propensity_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick check for whether we've fit the propensity score correctly is to verify that the average propensity score is equal to the probability of treatment in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_treat = learning_df['intervention'].mean()\n",
    "\n",
    "assert np.isclose(learning_df['propensity_score'].mean(), p_treat, atol=0.01), \"The average propensity score is not equal to the probability of treatment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Propensity score give us a lot of flexibility in visualizing confounding issues. For example, we can generate the same barplot we made in part 1 with `success_expect` on the x-axis, but with the `propensity_score` on the y-axis for our `learning_df` dataset.\n",
    "\n",
    "What do you observe about the relationship between `success_expect` and `propensity_score`?\n",
    "\n",
    "**Your response:** [pollev.com/tliu](https://pollev.com/tliu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO a barplot of x=success_expect vs y=propensity_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Propensity scores for exploring covariate distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Project 1, we plotted the distribution of covariates of treated and control units to visually verify that the covariates were balanced between the two groups.\n",
    "\n",
    "The propensity score is a convenient way check whether **all** the covariates we want to control for are balanced between the two groups.\n",
    "\n",
    "Run the cell below to fit a propensity score model for our simulated data from Part 1. Then plot the distribution of propensity scores for the treated and control units by creating a [sns.kdeplot](https://seaborn.pydata.org/generated/seaborn.kdeplot.html) with the x-axis as the `propensity_score` and the hue as the `sim_treat` variable -- optionally setting `cumulative=True` to plot the cumulative distribution. The distributions should approximately match since we simulated a randomized experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_propensity_model = smf.logit('sim_treat ~ 1 + sim_success_expect', data=sim_df).fit()\n",
    "sim_propensity_scores = sim_propensity_model.predict(sim_df)\n",
    "sim_df['propensity_score'] = sim_propensity_scores\n",
    "\n",
    "# TODO kdeplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create the same plot for our `learning_df` dataset, setting the hue to `intervention` and `x` to `propensity_score`.\n",
    "\n",
    "Differences in the distributions suggest that the covariates are not balanced between the two groups -- in other words, there is confounding among the covariates we've included in the propensity score model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(x='propensity_score', data=learning_df, hue='intervention')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of density plot of the propensity score also allows us to check positivity -- if there are regions of the propensity score where there are only treated or only control units, then there is a positivity violation.\n",
    "\n",
    "Does there appear to be any obvious regions where there are only treated or only control units? What about regions where there are few treated or control units?\n",
    "\n",
    "**Your response:** [pollev.com/tliu](https://pollev.com/tliu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Propensity scores as a balancer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the propensity score distilling high-dimensional covariates into a single number, binning the data by the propensity score becomes a viable strategy for balancing the covariates.\n",
    "\n",
    "Run the cell below to bin the data by the propensity score using the [pd.cut](https://pandas.pydata.org/docs/reference/api/pandas.cut.html) function, producing a new column `propensity_score_bin` in the `learning_df` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 5\n",
    "learning_df['propensity_score_bin'] = pd.cut(learning_df['propensity_score'], bins=n_bins)\n",
    "\n",
    "# view the bins created by pd.cut\n",
    "print(learning_df['propensity_score_bin'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, perform a groupby with `by='[propensity_score_bin', 'intervention']`, and calculate the mean of the given columns on the grouped data.\n",
    "\n",
    "Are there any of the columns that are significantly different between the $T=1$ and $T=0$ groups for each propensity score bin? If so, how do you interpret this?\n",
    "\n",
    "**Your response:** [pollev.com/tliu](https://pollev.com/tliu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO group by propensity score bin and intervention, and calculate the mean of the columns on the grouped data\n",
    "columns = ['success_expect', 'frst_in_family', 'school_urbanicity', 'gender', 'achievement_score']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
