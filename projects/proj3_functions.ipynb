{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 Part 1: Functions ðŸ’‰\n",
    "\n",
    ":::{epigraph}\n",
    "Instrumental Variables\n",
    "\n",
    "-- TODO your name here\n",
    ":::\n",
    "\n",
    ":::{admonition} Collaboration Statement\n",
    "- TODO brief statement on the nature of your collaboration.\n",
    "- TODO your collaborator's names here\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{tip}\n",
    "\n",
    "You'll notice that the non-function cells of this notebook are contained in a `if __name__ == \"__main__\":` block. This indicates to Jupyter that the code should only be run when the file is executed directly, not when it is imported as a module. If you write additional tests or cells that are not functions, make sure to add them within a `if __name__ == \"__main__\":` block as well.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Rubric\n",
    "\n",
    "| Section | Points |\n",
    "|------------------------------------|-------|\n",
    "| Instrumental variable estimators | 3 |\n",
    "| Characterizing compliers | 0.5 |\n",
    "| Assumption violation simulation | 1.5 |\n",
    "| Assumption violation widget | 1.5 |\n",
    "| Total | 6.5 pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from ipywidgets import interact_manual\n",
    "\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Instrumental Variable Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll begin by implementing two instrumental variable estimators: the Wald estimator and the two-stage least squares (TSLS) estimator, with covariates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Wald Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We saw in class that the Wald estimator is given by:\n",
    "\n",
    "$$\n",
    "\\frac{E[Y | Z = 1] - E[Y | Z = 0]}{E[T | Z = 1] - E[T | Z = 0]}\n",
    "$$\n",
    "\n",
    "This quantity can be interpreted as the ATE under the linear outcome assumption, or the LATE under the monotonicity assumption. It can also be expressed as follows:\n",
    "\n",
    "$$\n",
    "\\frac{E[Y | Z = 1] - E[Y | Z = 0]}{E[T | Z = 1] - E[T | Z = 0]} = \\frac{ITT}{P(\\text{compliers})}\n",
    "$$\n",
    "\n",
    "where the ITT is the intent-to-treat effect of the instrument on the outcome:\n",
    "\n",
    "\n",
    "$$\n",
    "ITT = E[Y | Z = 1] - E[Y | Z = 0]\n",
    "$$\n",
    "\n",
    "and the $P(\\text{compliers})$ is the proportion of compliers in the sample. See [Activity 12](https://comsc341cd.github.io/activities/activity12.html#activity12) for additional references on how we computed the proportion of compliers, never takers, and always takers:\n",
    "\n",
    "\n",
    "$$\n",
    "P(\\text{never takers}) = E[T=0 | Z = 1]\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{always takers}) = E[T=1 | Z = 0]\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{compliers}) = 1 - P(\\text{never takers}) - P(\\text{always takers})\n",
    "$$\n",
    "\n",
    "Complete the functions below to implement the Wald estimator and related quantities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{tip}\n",
    "\n",
    "With pandas and numpy logical indexing, the implementation of these functions should be brief and not need any for loops.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intent_to_treat(df, iv_col, outcome_col):\n",
    "    \"\"\"\n",
    "    Calculate the intent-to-treat (ITT) estimate of iv_col on outcome_col.\n",
    "\n",
    "    This should look very similar to our diff_in_means function from Project 1, but with a slightly different function signature.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataframe containing the treatment and outcome variables.\n",
    "        iv_col (str): The name of the instrument variable in the dataframe.\n",
    "        outcome_col (str): The name of the outcome variable in the dataframe.\n",
    "    \n",
    "    Returns:\n",
    "        float: The ITT estimate of iv_col on outcome_col.\n",
    "    \"\"\"\n",
    "    # TODO your code here\n",
    "    pass\n",
    "\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "    test_df = pd.DataFrame({\n",
    "        'T': [1, 0, 1, 0],\n",
    "        'Z': [1, 0, 1, 1],\n",
    "        'Y': [2, 1, 3, 2]\n",
    "    })\n",
    "\n",
    "    assert np.isclose(intent_to_treat(test_df, 'Z', 'Y'), (3+2+2)/3 - 1), \"ITT estimate is incorrect\"\n",
    "    \n",
    "    # these tests are not exhaustive, so adding additional asserts is recommended\n",
    "    print(\"All asserts for intent_to_treat passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prop_never_takers(df, treat_col, instrument_col):\n",
    "    \"\"\"\n",
    "    Calculate the proportion of never takers in the dataset.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataframe containing the treatment, outcome, and instrument variables.\n",
    "        treat_col (str): The name of the treatment variable in the dataframe.\n",
    "        instrument_col (str): The name of the instrument variable in the dataframe.\n",
    "    \n",
    "    Returns:\n",
    "        float: The proportion of never takers in the dataset.\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO your code here\n",
    "    pass\n",
    "\n",
    "\n",
    "def prop_always_takers(df, treat_col, instrument_col):\n",
    "    \"\"\"\n",
    "    Calculate the proportion of always takers in the dataset.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataframe containing the treatment and instrument variables.\n",
    "        treat_col (str): The name of the treatment variable in the dataframe.\n",
    "        instrument_col (str): The name of the instrument variable in the dataframe.\n",
    "    \n",
    "    Returns:\n",
    "        float: The proportion of always takers in the dataset.\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO your code here\n",
    "    pass\n",
    "    \n",
    "\n",
    "def prop_compliers(df, treat_col, instrument_col):\n",
    "    \"\"\"\n",
    "    Calculate the proportion of compliers in the dataset.\n",
    "\n",
    "    This can be computed either as:\n",
    "\n",
    "    E[T=1 | Z=1] - E[T=1 | Z=0]\n",
    "    or\n",
    "    1 - P(never takers) - P(always takers)\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataframe containing the treatment and instrument variables.\n",
    "        treat_col (str): The name of the treatment variable in the dataframe.\n",
    "        instrument_col (str): The name of the instrument variable in the dataframe.\n",
    "    \n",
    "    Returns:\n",
    "        float: The proportion of compliers in the dataset.\n",
    "    \"\"\"\n",
    "    # TODO your code here\n",
    "    pass\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_df = pd.DataFrame({\n",
    "        'T': [1, 0, 1, 0],\n",
    "        'Z': [1, 0, 1, 1],\n",
    "        'Y': [2, 1, 3, 2]\n",
    "    })\n",
    "\n",
    "    assert np.isclose(prop_always_takers(test_df, 'T', 'Z'), 0), \"prop_always_takers should be 0\"\n",
    "    assert np.isclose(prop_never_takers(test_df, 'T', 'Z'), 1/3), \"prop_never_takers should be 1/3\"\n",
    "    assert np.isclose(prop_compliers(test_df, 'T', 'Z'), 2/3), \"prop_compliers should be 2/3\"\n",
    "\n",
    "    # these tests are not exhaustive, so adding additional asserts is recommended\n",
    "    print(\"All asserts for compliance status proportions passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wald_estimator(df, treat_col='T', outcome_col='Y', instrument_col='Z'):\n",
    "    \"\"\"\n",
    "    Calculate the Wald estimator for the given dataset.\n",
    "\n",
    "    Can be implemented in a number of different ways, including:\n",
    "    - direct calculation\n",
    "    - using the intent_to_treat and prop_compliers functions\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataframe containing the treatment, outcome, and instrument variables.\n",
    "        treat_col (str): The name of the treatment variable in the dataframe.\n",
    "        outcome_col (str): The name of the outcome variable in the dataframe.\n",
    "        instrument_col (str): The name of the instrument variable in the dataframe.\n",
    "    \n",
    "    Returns:\n",
    "        float: The Wald estimator for the given dataset.\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO your code here\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_df = pd.DataFrame({\n",
    "        'T': [1, 0, 1, 0],\n",
    "        'Z': [1, 0, 1, 1],\n",
    "        'Y': [2, 1, 3, 2]\n",
    "    })\n",
    "\n",
    "    assert np.isclose(wald_estimator(test_df, 'T', 'Y', 'Z'),  (4/3) / (2/3)), \"wald_estimator is incorrect\"\n",
    "    \n",
    "    print(\"All asserts for wald_estimator passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Two-Stage Least Squares (TSLS) estimator with covariates\n",
    "\n",
    "The two-stage least squares estimate that we implemented in [Worksheet 5](https://comsc341cd.github.io/worksheets/worksheet5.html) will produce the same estimate as the Wald estimator. \n",
    "\n",
    "However, the advantage of the TSLS estimator is that it can be extended to include covariates, which is critical for the case when the instrument unconfoundedness assumption is satisfied only when conditioning on covariates. This is a so-called \"conditional instrument\" as shown on slide 14 of [Lecture 15](https://moodle.mtholyoke.edu/pluginfile.php/1456609/mod_resource/content/1/lec15-instrumental-variables-i.pdf). \n",
    "\n",
    "Specifically, we can fit the following models for the first and second stages:\n",
    "\n",
    "$$\n",
    "T = \\alpha_0 + \\alpha_1 Z + \\alpha_2 X\n",
    "$$\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1 \\hat{T} + \\beta_2 X\n",
    "$$\n",
    "\n",
    "with $\\hat{T}$ being the predicted treatment from the first stage, and $\\beta_1$ being the causal effect of interest. Similar to our regresion implementations for observational studies, the covariates $X$ are confounders between the instrument and the outcome, and we can include multiple covariates in the model.\n",
    "\n",
    "Even in the case where the instrument is completely randomized, the inclusion of covariates can still improve the precision of the causal effect estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsls_with_covariates(df, treat_col='T', outcome_col='Y', instrument_col='Z', covariates=None):\n",
    "    \"\"\"\n",
    "    Calculate the two-stage least squares estimate for the given dataset, optionally conditioning on covariates.\n",
    "\n",
    "    This function is an extension of the two-stage least squares estimator implemented in WS 5, so \n",
    "    you can use that implementation as a starting point.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataframe containing the treatment, outcome, and instrument variables.\n",
    "        treat_col (str): The name of the treatment variable in the dataframe.\n",
    "        outcome_col (str): The name of the outcome variable in the dataframe.\n",
    "        instrument_col (str): The name of the instrument variable in the dataframe.\n",
    "        covariates (list[str]): The covariates to condition on, defaults to None.\n",
    "    \n",
    "    Returns:\n",
    "        float: The two-stage least squares estimate for the given dataset.\n",
    "    \"\"\"\n",
    "    # TODO fit the first stage model, optionally conditioning on covariates\n",
    "\n",
    "    # TODO add the predicted treatment values to the dataframe\n",
    "\n",
    "    # TODO fit the second stage model, optionally conditioning on covariates\n",
    "\n",
    "    # TODO return the causal effect estimate from the second stage model\n",
    "    pass\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Test the function with a simple dataset\n",
    "    df = pd.DataFrame({\n",
    "        'T': [1, 0, 1, 0],\n",
    "        'Z': [1, 0, 1, 1],\n",
    "        'Y': [2, 1, 3, 2],\n",
    "        'X': [1, 1, 1, 1]\n",
    "    })\n",
    "\n",
    "    # Test without covariates, should be the same as the Wald estimator\n",
    "    assert np.isclose(tsls_with_covariates(df), wald_estimator(df)), \"tsls should be the same as wald_estimator\"\n",
    "\n",
    "    # Test with including a constant covariate\n",
    "    assert np.isclose(tsls_with_covariates(df, covariates=['X']), (4/3) / (2/3))\n",
    "\n",
    "    # these tests are not exhaustive, so adding additional asserts is recommended\n",
    "    print(\"All asserts for tsls_with_covariates passed!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Characterizing Compliers\n",
    "\n",
    "A fundamental challenge in instrumental variable estimation is that we do not know whether an individual unit is a complier or not. That is because of how **potential** treatment status corresponds to the **observed** treatment status in IV studies.\n",
    "\n",
    "We have the following table for the observed instrument and treatment status, assuming monotonicity (no defiers):\n",
    "\n",
    "| Z | T | Compliance status |\n",
    "|---|---|-------------------|\n",
    "| 0 | 0 | Never taker or complier |\n",
    "| 0 | 1 | Always taker |\n",
    "| 1 | 0 | Never taker |\n",
    "| 1 | 1 | Always taker or complier |\n",
    "\n",
    "\n",
    "- In the case where $Z=0$ and $T=0$, we can't tell whether the unit is a complier, or a never taker who just happened to be in the $Z=0$ group.\n",
    "\n",
    "- In the case where $Z=1$ and $T=1$, we can't tell whether the unit is a complier, or an always taker who just happened to be in the $Z=1$ group.\n",
    "\n",
    "Instead, we have to rely on summary statistics to characterize compliers in the sample. For example, if age is a covariate in our study, there are ways to estimate the average age of compliers in the sample, which we can then compare to the average age of all units in the sample.\n",
    "\n",
    "We will use the following formula from [Marbach and Hangartner 2020](https://www.cambridge.org/core/services/aop-cambridge-core/content/view/0E2AA51F9BD436A5DB5A84462732A9C3/S1047198719000482a.pdf/profiling-compliers-and-noncompliers-for-instrumental-variable-analysis.pdf) to estimate $E[X \\mid \\text{complier}]$, where $X$ is a covariate of interest:\n",
    "\n",
    "\n",
    "$$\n",
    "E[X \\mid \\text{complier}] = \\frac{1}{P(\\text{complier})} E[X] - \\frac{P(\\text{never taker})}{P(\\text{complier})} E[X \\mid \\text{never taker}] - \\frac{P(\\text{always taker})}{P(\\text{complier})} E[X \\mid \\text{always taker}]\n",
    "$$\n",
    "\n",
    "This quantity can be understood as a \"weighted average\" of the covariate $X$ across the three compliance statuses, where the weights are inversely proportional to the compliance status proportions.\n",
    "\n",
    "The conditional means in the expression above can be estimated by:\n",
    "\n",
    "1. selecting the subset of a dataframe that are never takers ($T=0$ AND $Z=1$) or always takers ($T=1$ AND $Z=0$)\n",
    "2. calculating the mean of $X$ in the selected subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complier_covariate_mean(df, covariate_col, treat_col, instrument_col):\n",
    "    \"\"\"\n",
    "    Calculate the mean of a covariate among compliers in the dataset.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataframe containing the treatment, instrument, and covariate variables.\n",
    "        covariate_col (str): The name of the covariate variable in the dataframe.\n",
    "        treat_col (str): The name of the treatment variable in the dataframe.\n",
    "        instrument_col (str): The name of the instrument variable in the dataframe.\n",
    "    \n",
    "    Returns:\n",
    "        float: The mean of the covariate among compliers in the dataset.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # TODO select the subset of units that are never takers (nt) or always takers (at)\n",
    "    nt_df = None\n",
    "    at_df = None\n",
    "\n",
    "    # TODO calculate the mean of the covariate in the selected subsets\n",
    "    nt_mean = None\n",
    "    at_mean = None\n",
    "\n",
    "    # TODO calculate the overall mean\n",
    "    overall_mean = None\n",
    "\n",
    "    # TODO calculate the compliance status proportions\n",
    "    prop_nt = None\n",
    "    prop_at = None\n",
    "    prop_co = None\n",
    "\n",
    "    # TODO calculate the complier mean\n",
    "    complier_mean = None\n",
    "\n",
    "    return complier_mean\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Test the function with a simple dataset\n",
    "    df = pd.DataFrame({\n",
    "        'Z': [1, 0, 1, 0, 1, 0,],\n",
    "        'T': [0, 0, 1, 1, 1, 0,],\n",
    "        'Y': [2, 1, 3, 2, 5, 4,],\n",
    "        'X': [1, 2, 3, 4, 5, 5,]        \n",
    "    })\n",
    "    assert np.isclose(complier_covariate_mean(df, 'X', 'T', 'Z'), 5), \"complier_covariate_mean should be 5\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Simulation study of IV assumption violations\n",
    "\n",
    "We'll now extend the simulation study that generates $Z$, $X$, $T$, and $Y$ from Worksheet 5 to include violations of the following 3 IV assumptions:\n",
    "\n",
    "- instrument unconfoundedness\n",
    "- relevance\n",
    "- exclusion restriction\n",
    "\n",
    "Additionally, we'll quantify the bias and variance for such violations. \n",
    "\n",
    "## 3.1 Building the simulation\n",
    "\n",
    "The first variable we'll generate is a binary unobserved confounder $X$:\n",
    "\n",
    "$$\n",
    "X \\sim \\text{Bernoulli}(0.5)\n",
    "$$\n",
    "\n",
    "This corresponds to:\n",
    "\n",
    "```python\n",
    "X = rng.choice([0, 1], size=n_samples)\n",
    "```\n",
    "\n",
    ":::{note}\n",
    "In this simulation, we'll make the confounder $X$ observable so that we can include it as a covariate in the two-stage least squares estimation.\n",
    ":::\n",
    "\n",
    "### Instrument Unconfoundedness\n",
    "\n",
    "We'll next generate a binary instrument $Z$ in the same way as $X$:\n",
    "\n",
    "$$\n",
    "Z \\sim \\text{Bernoulli}(0.5)\n",
    "$$\n",
    "\n",
    "To enable a violation of the instrument unconfoundedness assumption, we include a boolean `unconfoundedness_violation` parameter, with the following effect on the simulation:\n",
    "\n",
    "$$\n",
    "Z = \\begin{cases} \n",
    "    Z + X > 1 & \\text{if } \\text{unconfoundedness violation = True} \\\\\n",
    "    Z & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Thus, if `unconfoundedness_violation = True`, the instrument $Z$ is affected by the confounder $X$.\n",
    "\n",
    "### Relevance\n",
    "\n",
    "To strengthen or weaken the relevance assumption, we can manipulate the **strength** of the relationship between the instrument and the treatment, which we call $\\pi$ below. Specifically, $\\pi$ will take on values from 0 to 1, where 0 is a total violation of the relevance assumption and 1 is a \"fully relevant\" relationship. Modify the simulation from [Worksheet 5](https://comsc341cd.github.io/worksheets/ws5.html#simulating-an-iv-study-0-5-pts) to include this parameter as follows:\n",
    "\n",
    "$$\n",
    "T = \\begin{cases} \n",
    "    1 & \\text{if } \\pi Z + X + \\epsilon > 1\\\\\n",
    "    0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\pi$ is the strength of the relationship between the instrument and the treatment, `instrument_strength` in the function\n",
    "- $\\epsilon \\sim \\text{N}(0, 1)$ is the noise term, sampled from a normal distribution with mean 0 and standard deviation 1\n",
    "\n",
    "### Exclusion Restriction\n",
    "\n",
    "Finally, to enable a violation of the exclusion restriction, we include a boolean `exclusion_restriction_violation` parameter, with the following effect on the simulation:\n",
    "\n",
    "$$\n",
    "Y = \\begin{cases} \n",
    "    \\tau T + \\gamma X + Z + \\epsilon & \\text{if } \\text{exclusion restriction violation = True} \\\\\n",
    "    \\tau T + \\gamma X + \\epsilon & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\tau$ is the treatment effect, `treatment_effect` in the function\n",
    "- $\\gamma$ is the effect of the confounder $X$ on the outcome, `confounder_effect` in the function\n",
    "- $\\epsilon \\sim \\text{N}(0, 1)$ is the noise term, sampled from a normal distribution with mean 0 and standard deviation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_iv_assumptions(n_samples=1000, treatment_effect=1, \n",
    "                       confounder_effect=3, \n",
    "                       instrument_strength=1, \n",
    "                       exclusion_restriction_violation=False, \n",
    "                       unconfoundedness_violation=False):\n",
    "    \"\"\"\n",
    "    Simulate an IV study with a binary instrument and confounded treatment and outcome, \n",
    "    with potential violations of the relevance, exclusion restriction, and unconfoundedness assumptions.\n",
    "\n",
    "    Args:\n",
    "        n_samples (int): The number of samples to simulate, defaults to 10000\n",
    "        treatment_effect (float): The effect of the treatment on the outcome, defaults to 1\n",
    "        confounder_effect (float): The effect of the confounder on the outcome, defaults to 3\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame with the simulated data:\n",
    "            - T: The treatment variable\n",
    "            - Z: The instrument variable\n",
    "            - Y: The outcome variable\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO generate binary confounder \n",
    "    X = 0\n",
    "\n",
    "    # TODO generate binary instrument\n",
    "    Z = 0\n",
    "\n",
    "    # TODO update Z based on unconfoundedness_violation\n",
    "\n",
    "    # TODO generate treatment dependent on Z, X, and noise, incorporating instrument_strength\n",
    "    T = 0\n",
    "\n",
    "    # TODO generate outcome dependent on T, X, Z, and noise, incorporating treatment_effect and confounder_effect\n",
    "    Y = 0\n",
    "\n",
    "    # TODO update Y based on exclusion_restriction_violation\n",
    "    #return pd.DataFrame({'X': X, 'Z': Z, 'T': T, 'Y': Y})\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Test the function with no violations\n",
    "    df = sim_iv_assumptions(n_samples=1000, treatment_effect=1, confounder_effect=3, instrument_strength=1, exclusion_restriction_violation=False, unconfoundedness_violation=False)\n",
    "    assert df[['X', 'Z']].corr().iloc[0, 1] < 0.1, \"X and Z should be relatively uncorrelated with no instrument unconfoundedness violation\"\n",
    "    \n",
    "    # make sure Z and T are integers\n",
    "    assert df['Z'].dtype == int, \"Z should be an integer\"\n",
    "    assert df['T'].dtype == int, \"T should be an integer\"\n",
    "\n",
    "    # Test the function with instrument unconfoundedness violation\n",
    "    df = sim_iv_assumptions(n_samples=1000, treatment_effect=1, confounder_effect=3, instrument_strength=1, exclusion_restriction_violation=False, unconfoundedness_violation=True)\n",
    "    assert df[['X', 'Z']].corr().iloc[0, 1] > 0.4, \"X and Z should be positively correlated with instrument unconfoundedness violation\"\n",
    "    \n",
    "    # Test the function with relevance violation\n",
    "    df = sim_iv_assumptions(n_samples=1000, treatment_effect=1, confounder_effect=3, instrument_strength=0, exclusion_restriction_violation=False, unconfoundedness_violation=False)\n",
    "    assert df[['T', 'Z']].corr().iloc[0, 1] < 0.1, \"T and Z should be relatively uncorrelated with instrument_strength = 0\"\n",
    "    \n",
    "    # these tests are not exhaustive, so adding additional asserts is recommended\n",
    "    print(\"All asserts for sim_iv_assumptions passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Quantifying the bias and variance of treatment effect estimates\n",
    "\n",
    "In addition to visually inspecting the bias and variance of the treatment effect estimate as we've done throughout the semester, we can also quantify the bias and variance of the treatment effect estimate.\n",
    "\n",
    "### Bias\n",
    "\n",
    "The bias of a treatment effect estimate can be calculated as follows:\n",
    "\n",
    "$$\n",
    "\\text{bias} = E[\\hat{\\tau}] - \\tau\n",
    "$$\n",
    "\n",
    "Where $\\tau$ is the true treatment effect and $E[\\hat{\\tau}]$ is the expected value (mean) of the treatment effect estimate.\n",
    "\n",
    "### Variance\n",
    "\n",
    "The variance of a treatment effect estimate can be calculated as follows:\n",
    "\n",
    "$$\n",
    "\\text{variance} = E[(\\hat{\\tau} - E[\\hat{\\tau}])^2]\n",
    "$$\n",
    "\n",
    "This can be conveniently calculated using [np.var](https://numpy.org/doc/stable/reference/generated/numpy.var.html):\n",
    "\n",
    "```python\n",
    "np.var(estimated_effects_list)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias(estimated_effects_list, true_effect):\n",
    "    \"\"\"\n",
    "    Calculate the bias of a treatment effect estimate.\n",
    "\n",
    "    Args:\n",
    "        estimated_effects_list (list): A list of estimated treatment effect estimates\n",
    "        true_effect (float): The true treatment effect\n",
    "\n",
    "    Returns:\n",
    "        float: The bias of the treatment effect estimate\n",
    "    \"\"\"\n",
    "    # TODO your code here\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    true_effect = 1\n",
    "    estimated_effects_list = [0, 1, 0, 1]\n",
    "    assert np.isclose(bias(estimated_effects_list, true_effect), -0.5), \"Bias should be -0.5\"\n",
    "\n",
    "    # these tests are not exhaustive, so adding additional asserts is recommended\n",
    "    print(\"All asserts for bias passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Assumption violation simulation widget\n",
    "\n",
    "Finally, we'll extend the `plot_iv_estimates` function from [Worksheet 5](https://comsc341cd.github.io/worksheets/ws5.html#estimating-causal-effects-with-an-iv-0-5-pts) where we compared naive regression to IV estimation.\n",
    "The widget should have interactive sliders for the strength of the instrument, as well as toggles for the violations of the instrument unconfoundedness and exclusion restriction assumptions. Additionally, the widget should have a toggle for whether to include the confounder $X$ as a covariate in the two-stage least squares estimation.\n",
    "\n",
    "**Make sure to limit the range of the instrument strength slider to 0 to 1 with step size of 0.1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_regression(df):\n",
    "    \"\"\"\n",
    "    Perform a regression on the treatment, which is incorrect due to the missing confounder.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe containing the data\n",
    "    \"\"\"\n",
    "    naive_formula = 'Y ~ 1 + T'\n",
    "    naive_model = smf.ols(naive_formula, data=df).fit()\n",
    "    return naive_model.params['T']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add interact_manual decorator, with iv_strength slider of range 0 to 1 and step size of 0.1\n",
    "def plot_iv_estimate_assumptions(iv_strength=0.5, unconfoundedness_violation=False, exclusion_restriction_violation=False, include_confounder=False):\n",
    "    \"\"\"ArithmeticError\n",
    "    \n",
    "    Plots the distribution of the IV and naive estimates of the treatment effect for the given assumptions.\n",
    "    Additionally displays the bias and variance of the IV and naive estimates.\n",
    "\n",
    "    Holds the treatment effect constant at 1, and the confounder effect at 5.\n",
    "\n",
    "    Args:\n",
    "        iv_strength (float): The strength of the instrument, defaults to 0.5\n",
    "        unconfoundedness_violation (bool): Whether to include a violation of the instrument unconfoundedness assumption, defaults to False\n",
    "        exclusion_restriction_violation (bool): Whether to include a violation of the exclusion restriction assumption, defaults to False\n",
    "        include_confounder (bool): Whether to include the confounder X as a covariate in the two-stage least squares estimation, defaults to False\n",
    "    \"\"\"\n",
    "\n",
    "    treatment_effect = 1\n",
    "    confounder_effect = 5\n",
    "    n_datasets = 1000\n",
    "\n",
    "    iv_estimates = []\n",
    "    naive_estimates = []\n",
    "\n",
    "    fig = plt.figure()\n",
    "\n",
    "    for _ in range(n_datasets):\n",
    "        # Simulate an IV dataset with the given assumptions\n",
    "        iv_df = sim_iv_assumptions(treatment_effect=treatment_effect, \n",
    "                                   confounder_effect=confounder_effect, \n",
    "                                   instrument_strength=iv_strength, \n",
    "                                   exclusion_restriction_violation=exclusion_restriction_violation, \n",
    "                                   unconfoundedness_violation=unconfoundedness_violation)\n",
    "\n",
    "        # TODO compute the causal effect estimate from the naive regression\n",
    "        naive_estimates.append(\"TODO\")\n",
    "\n",
    "        # TODO compute the causal effect estimate using tsls, optionally including the confounder X as a covariate if include_confounder      \n",
    "        covariates = None\n",
    "        iv_estimates.append(\"TODO\")\n",
    "\n",
    "    # Plot the true effect\n",
    "    plt.axvline(treatment_effect, color='black', linestyle='--', label='True Effect')\n",
    "\n",
    "    # TODO plot kdeplots of IV estimates and naive estimates\n",
    "    \n",
    "\n",
    "\n",
    "    # TODO annotate the title with the bias and variance of the IV and naive estimates\n",
    "    iv_bias = None\n",
    "    iv_variance = None\n",
    "    regression_bias = None\n",
    "    regression_variance = None\n",
    "    plt.title(f'IV Bias: {iv_bias:.2f}, IV Variance: {iv_variance:.2f}\\n Regression Bias: {regression_bias:.2f}, Regression Variance: {regression_variance:.2f}')\n",
    "\n",
    "    plt.xlim(-9, 11)\n",
    "    plt.xticks(range(-9, 12, 1))\n",
    "\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{tip}\n",
    "It may be helpful to refer to the instrumental variable DAG when thinking about why you see certain changes in the bias and variance in the widget.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Begin by using the default values for the widget, where `instrument_strength = 0.5`, `unconfoundedness_violation = False`, `exclusion_restriction_violation = False`, and `include_confounder = False`. Compare the bias and variance of the IV and regression estimates and briefly describe what you observe.\n",
    "\n",
    "2. Now vary the `instrument_strength` slider. Describe what you observe in terms of bias and variance when you:\n",
    "\n",
    "- increase the strength of the instrument\n",
    "- decrease the strength of the instrument\n",
    "- set the strength of the instrument to the extremes of 0 and 1\n",
    "\n",
    "3. Next, set the `instrument_strength` to 0.5 and toggle the `exclusion_restriction_violation` checkbox. When this assumption is violated, does it primarily affect the bias or the variance of the treatment effect estimate? \n",
    "\n",
    "4. Similarly, reset the `instrument_strength` to 0.5 and toggle the `unconfoundedness_violation` checkbox. When this assumption is violated, does it primarily affect the bias or the variance of the treatment effect estimate? What do you observe when you introduce `include_confounder = True`, and why does this occur?\n",
    "\n",
    "5. Finally, consider a valid IV setting, where there is no violation of the instrument unconfoundedness or exclusion restriction assumptions, and the instrument is strong (highly relevant to the treatment). Describe what happens to the variance of the IV estimate when you set `include_confounder = True`, and briefly discuss why you think this occurs.\n",
    "\n",
    "**TODO your responses below:**\n",
    "\n",
    "1.\n",
    "2.\n",
    "3.\n",
    "4.\n",
    "5."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
